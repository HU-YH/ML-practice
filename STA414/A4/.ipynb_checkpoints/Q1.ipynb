{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Unsupervised Learning"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import scipy\n",
    "import scipy.stats\n",
    "import numpy as np\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Generating the data\n",
    "\n",
    "First, we will generate some data for this problem. Set the number of points $N=400$, their dimension $D=2$, and the number of clusters $K=2$, and generate data from the distribution $p(x|z=k) = \\mathcal{N}(\\mu_k, \\Sigma_k)$.\n",
    "  Sample $200$ data points for $k=1$ and 200 for $k=2$, with\n",
    "\n",
    "  $$\n",
    "    \\mu_1=\n",
    "    \\begin{bmatrix}\n",
    "      0.1 \\\\\n",
    "      0.1\n",
    "    \\end{bmatrix}\n",
    "    \\ \\text{,}\\\n",
    "    \\mu_2=\n",
    "    \\begin{bmatrix}\n",
    "      6.0 \\\\\n",
    "      0.1\n",
    "    \\end{bmatrix}\n",
    "    \\ \\text{ and }\\\n",
    "    \\Sigma_1=\\Sigma_2=\n",
    "    \\begin{bmatrix}\n",
    "      10       & 7 \\\\\n",
    "      7 & 10\n",
    "    \\end{bmatrix}\n",
    "  $$\n",
    "  Here, $N=400$. Since you generated the data, you already know which sample comes from which class.\n",
    "  Run the cell in the IPython notebook to generate the data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num_samples = 400\n",
    "cov = np.array([[1., .7], [.7, 1.]]) * 10\n",
    "mean_1 = [.1, .1]\n",
    "mean_2 = [6., .1]\n",
    "\n",
    "x_class1 = np.random.multivariate_normal(mean_1, cov, num_samples // 2)\n",
    "x_class2 = np.random.multivariate_normal(mean_2, cov, num_samples // 2)\n",
    "xy_class1 = np.column_stack((x_class1, np.zeros(num_samples // 2)))\n",
    "xy_class2 = np.column_stack((x_class2, np.ones(num_samples // 2)))\n",
    "data_full = np.row_stack([xy_class1, xy_class2])\n",
    "np.random.shuffle(data_full)\n",
    "data = data_full[:, :2]\n",
    "labels = data_full[:, 2]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Make a scatter plot of the data points showing the true cluster assignment of each point using different color codes and shape (x for first class and circles for second class):"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(x_class1[:,0],x_class1[:,1],'X') # first class, x shape\n",
    "plt.plot(x_class2[:,0],x_class2[:,1],'o') # second class, circle shape\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Implement and Run K-Means algorithm\n",
    "\n",
    "Now, we assume that the true class labels are not known. Implement the k-means algorithm for this problem.\n",
    "  Write two functions: `km_assignment_step`, and `km_refitting_step` as given in the lecture (Here, `km_` means k-means).\n",
    "  Identify the correct arguments, and the order to run them. Initialize the algorithm with\n",
    "  $$\n",
    "    \\hat\\mu_1=\n",
    "    \\begin{bmatrix}\n",
    "      0.0 \\\\\n",
    "      0.0\n",
    "    \\end{bmatrix}\n",
    "    \\ \\text{,}\\\n",
    "    \\hat\\mu_2=\n",
    "    \\begin{bmatrix}\n",
    "      1.0 \\\\\n",
    "      1.0\n",
    "    \\end{bmatrix}\n",
    "  $$\n",
    "  and run it until convergence.\n",
    "  Show the resulting cluster assignments on a scatter plot either using different color codes or shape or both.\n",
    "  Also plot the cost vs. the number of iterations. Report your misclassification error."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def cost(data, R, Mu):\n",
    "    N, D = data.shape\n",
    "    K = Mu.shape[1]\n",
    "    J = 0\n",
    "    for k in range(K):\n",
    "        J += np.dot(np.linalg.norm(data - np.array([Mu[:, k], ] * N), axis=1)**2, R[:, k])\n",
    "    return J"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO: K-Means Assignment Step\n",
    "def km_assignment_step(data, Mu):\n",
    "    \"\"\" Compute K-Means assignment step\n",
    "    \n",
    "    Args:\n",
    "        data: a NxD matrix for the data points\n",
    "        Mu: a DxK matrix for the cluster means locations\n",
    "    \n",
    "    Returns:\n",
    "        R_new: a NxK matrix of responsibilities\n",
    "    \"\"\"\n",
    "    \n",
    "    # Fill this in:\n",
    "    N, D = data.shape\n",
    "    K = Mu.shape[1]\n",
    "    r = np.zeros((N,K))\n",
    "    for k in range(K):\n",
    "        r[:, k] = np.linalg.norm(data-Mu[:,k],ord=2,axis=1)**2\n",
    "    arg_min = np.argmin(r,axis=1) # argmax/argmin along dimension 1\n",
    "    R_new = np.zeros((N,K)) # Set to zeros/ones with shape (N, K)\n",
    "    R_new[np.arange(0,N,1), arg_min] = 1 # Assign to 1\n",
    "    return R_new"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def km_refitting_step(data, R, Mu):\n",
    "    \"\"\" Compute K-Means refitting step.\n",
    "    \n",
    "    Args:\n",
    "        data: a NxD matrix for the data points\n",
    "        R: a NxK matrix of responsibilities\n",
    "        Mu: a DxK matrix for the cluster means locations\n",
    "    \n",
    "    Returns:\n",
    "        Mu_new: a DxK matrix for the new cluster means locations\n",
    "    \"\"\"\n",
    "    N, D = data.shape # Number of datapoints and dimension of datapoint\n",
    "    K = Mu.shape[1]  # number of clusters\n",
    "    Mu_new =np.zeros((D,K))\n",
    "    for i in range(N):\n",
    "        Mu_new[:,R[i]]+=data[i]\n",
    "    Mu_new=Mu_new/np.sum(R,axis=0)\n",
    "    # np.sum(R,axis=0) returns\n",
    "    # an 1*k vector, number of datapoitns assigned to each class\n",
    "    return Mu_new"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "N, D = data.shape\n",
    "K = 2\n",
    "max_iter = 100\n",
    "class_init = np.random.binomial(1., .5, size=N)\n",
    "R = np.vstack([class_init, 1 - class_init]).T\n",
    "\n",
    "Mu = np.zeros([D, K])\n",
    "Mu[:, 1] = 1.\n",
    "R.T.dot(data), np.sum(R, axis=0)\n",
    "\n",
    "for it in range(max_iter):\n",
    "    R = km_assignment_step(data, Mu)\n",
    "    Mu = km_refitting_step(data, R, Mu)\n",
    "    print(it, cost(data, R, Mu))\n",
    "\n",
    "class_1 = np.where(R[:, 0])\n",
    "class_2 = np.where(R[:, 1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO: Make a scatterplot for the data points showing the K-Means cluster assignments of each point\n",
    "# plt.plot(...) # first class, x shape\n",
    "# plt.plot(...) # second class, circle shape\n",
    "correct_class_1 = num_samples // 2 - sum(labels[class_1])\n",
    "correct_class_2 = sum(labels[class_2])\n",
    "num_correct = correct_class_1 + correct_class_2\n",
    "print('Missclassification rate of K-mean clustering is ', round(1-num_correct/400,3)*100, '%')\n",
    "x_class1_new = data[class_1]\n",
    "x_class2_new = data[class_2]\n",
    "plt.plot(x_class1_new[:, 0], x_class1_new[:, 1], 'X')  # first class, x shape\n",
    "plt.plot(x_class2_new[:, 0], x_class2_new[:, 1], 'o')  # second class, circle shape\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Implement EM algorithm for Gaussian mixtures\n",
    "Next, implement the EM algorithm for Gaussian mixtures.\n",
    "  Write three functions: `log_likelihood`, `gm_e_step`, and `gm_m_step` as given in the lecture.\n",
    "  Identify the correct arguments, and the order to run them.\n",
    "  Initialize the algorithm with means as in Qs 2.1 k-means initialization, covariances with $\\hat\\Sigma_1=\\hat\\Sigma_2=I$,\n",
    "  and $\\hat\\pi_1=\\hat\\pi_2$.\n",
    "\n",
    "  In addition to the update equations in the lecture, for the M (Maximization) step, you also need to use this following equation to update the covariance $\\Sigma_k$:\n",
    "$$\\hat{\\mathbf{\\Sigma}_k} = \\frac{1}{N_k} \\sum^N_{n=1} r_k^{(n)}(\\mathbf{x}^{(n)} - \\hat{\\mathbf{\\mu}_k})(\\mathbf{x}^{(n)} - \\hat{\\mathbf{\\mu}_k})^{\\top}$$\n",
    "    \n",
    "  Run the algorithm until convergence and show the resulting cluster assignments on a scatter plot either using different color codes or shape or both.\n",
    "  Also plot the log-likelihood vs. the number of iterations. Report your misclassification error."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def normal_density(x, mu, Sigma):\n",
    "    return np.exp(-.5 * np.dot(x - mu, np.linalg.solve(Sigma, x - mu))) \\\n",
    "        / np.sqrt(np.linalg.det(2 * np.pi * Sigma))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def log_likelihood(data, Mu, Sigma, Pi):\n",
    "    \"\"\" Compute log likelihood on the data given the Gaussian Mixture Parameters.\n",
    "    \n",
    "    Args:\n",
    "        data: a NxD matrix for the data points\n",
    "        Mu: a DxK matrix for the means of the K Gaussian Mixtures\n",
    "        Sigma: a list of size K with each element being DxD covariance matrix\n",
    "        Pi: a vector of size K for the mixing coefficients\n",
    "    \n",
    "    Returns:\n",
    "        L: a scalar denoting the log likelihood of the data given the Gaussian Mixture\n",
    "    \"\"\"\n",
    "    # Fill this in:\n",
    "    N, D = data.shape  # Number of datapoints and dimension of datapoint\n",
    "    K = Mu.shape[1] # number of mixtures\n",
    "    L, T = 0., 0.\n",
    "    for n in range(N):\n",
    "        T = 0\n",
    "        for k in range(K):\n",
    "            #though we are a given normal_density function, I found there is a function with same functionality from scipy\n",
    "            T += Pi[k]*scipy.stats.multivariate_normal(mean=Mu[:,k],cov=Sigma[k]).pdf(data[n]) # Compute the likelihood from the k-th Gaussian weighted by the mixing coefficients\n",
    "        L += np.log(T)\n",
    "    return L"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO: Gaussian Mixture Expectation Step\n",
    "def gm_e_step(data, Mu, Sigma, Pi):\n",
    "    \"\"\" Gaussian Mixture Expectation Step.\n",
    "\n",
    "    Args:\n",
    "        data: a NxD matrix for the data points\n",
    "        Mu: a DxK matrix for the means of the K Gaussian Mixtures\n",
    "        Sigma: a list of size K with each element being DxD covariance matrix\n",
    "        Pi: a vector of size K for the mixing coefficients\n",
    "    \n",
    "    Returns:\n",
    "        Gamma: a NxK matrix of responsibilities \n",
    "    \"\"\"\n",
    "    # Fill this in:\n",
    "    N, D = data.shape  # Number of datapoints and dimension of datapoint\n",
    "    K = Mu.shape[1]  # number of mixtures\n",
    "    Gamma = np.zeros((N, K))  # zeros of shape (N,K), matrix of responsibilities\n",
    "    for n in range(N):\n",
    "        normal = np.zeros((2))\n",
    "        for k in range(K):\n",
    "            normal[k] = normal_density(data[n],Mu[:,k],Sigma[k])\n",
    "            Gamma[n, k] = Pi[k] * normal_density(data[n],Mu[:,k],Sigma[k])\n",
    "        Gamma[n, :] /= np.sum(Pi * normal) # Normalize by sum across second dimension (mixtures)\n",
    "\n",
    "    return Gamma"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO: Gaussian Mixture Maximization Step\n",
    "def gm_m_step(data, Gamma):\n",
    "    \"\"\" Gaussian Mixture Maximization Step.\n",
    "\n",
    "    Args:\n",
    "        data: a NxD matrix for the data points\n",
    "        Gamma: a NxK matrix of responsibilities \n",
    "    \n",
    "    Returns:\n",
    "        Mu: a DxK matrix for the means of the K Gaussian Mixtures\n",
    "        Sigma: a list of size K with each element being DxD covariance matrix\n",
    "        Pi: a vector of size K for the mixing coefficients\n",
    "    \"\"\"\n",
    "    # Fill this in:\n",
    "    N, D = data.shape  # Number of datapoints and dimension of datapoint\n",
    "    K = Gamma.shape[1]  # number of mixtures\n",
    "    Nk = np.sum(Gamma, axis=0)  # Sum along first axis\n",
    "    Mu = np.matmul(data.T, Gamma)/Nk\n",
    "    Sigma = np.zeros((K, D, D))\n",
    "\n",
    "    for k in range(K):\n",
    "        count = np.zeros((D,D))\n",
    "        for n in range(N):\n",
    "            residual = data[n]-Mu[:,k]\n",
    "            residual = np.reshape(residual,(D,-1))\n",
    "            residual_t = np.reshape(residual,(-1,D))\n",
    "            squared_residual = np.matmul(residual,residual_t)\n",
    "            count += Gamma[n,k]*squared_residual\n",
    "        Sigma[k] = count/Nk[k]\n",
    "    Sigma=Sigma\n",
    "    Pi = Nk / N\n",
    "    return Mu, Sigma, Pi"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO: Run this cell to call the Gaussian Mixture EM algorithm\n",
    "N, D = data.shape\n",
    "K = 2\n",
    "Mu = np.zeros([D, K])\n",
    "Mu[:, 1] = 1.\n",
    "Sigma = [np.eye(2), np.eye(2)]\n",
    "Pi = np.ones(K) / K\n",
    "Gamma = np.zeros([N, K]) # Gamma is the matrix of responsibilities \n",
    "\n",
    "max_iter  = 200\n",
    "\n",
    "for it in range(max_iter):\n",
    "    Gamma = gm_e_step(data, Mu, Sigma, Pi)\n",
    "    Mu, Sigma, Pi = gm_m_step(data, Gamma)\n",
    "    # print(it, log_likelihood(data, Mu, Sigma, Pi)) # This function makes the computation longer, but good for debugging\n",
    "\n",
    "class_1 = np.where(Gamma[:, 0] >= .5)\n",
    "class_2 = np.where(Gamma[:, 1] >= .5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO: Make a scatterplot for the data points showing the Gaussian Mixture cluster assignments of each point\n",
    "# plt.plot(...) # first class, x shape\n",
    "# plt.plot(...) # second class, circle shape\n",
    "correct_class_1 = num_samples // 2 - sum(labels[class_1])\n",
    "correct_class_2 = sum(labels[class_2])\n",
    "num_correct = correct_class_1 + correct_class_2\n",
    "print('Missclassification rate of EM algorithm is ', round(1-num_correct/400,3)*100, '%')\n",
    "x_class1_new = data[class_1]\n",
    "x_class2_new = data[class_2]\n",
    "plt.plot(x_class1_new[:, 0], x_class1_new[:, 1], 'X')  # first class, x shape\n",
    "plt.plot(x_class2_new[:, 0], x_class2_new[:, 1], 'o')  # second class, circle shape\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Comment on findings + additional experiments"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Comment on the results:\n",
    "\n",
    "* Compare the performance of k-Means and EM based on the resulting cluster assignments.\n",
    "* Compare the performance of k-Means and EM based on their convergence rate. What is the bottleneck for which method?\n",
    "* Experiment with 5 different data realizations (generate new data), run your algorithms, and summarize your findings. Does the algorithm performance depend on different realizations of data?\n",
    " \n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**TODO: Your written answer here**\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(\"###########################Start Experiement No.\",i+1,\"#######################################\")\n",
    "    num_samples = 400\n",
    "    cov = np.array([[1., .7], [.7, 1.]]) * 10\n",
    "    mean_1 = [.1, .1]\n",
    "    mean_2 = [6., .1]\n",
    "\n",
    "    x_class1 = np.random.multivariate_normal(mean_1, cov, num_samples // 2)\n",
    "    x_class2 = np.random.multivariate_normal(mean_2, cov, num_samples // 2)\n",
    "    xy_class1 = np.column_stack((x_class1, np.zeros(num_samples // 2)))\n",
    "    xy_class2 = np.column_stack((x_class2, np.ones(num_samples // 2)))\n",
    "    data_full = np.row_stack([xy_class1, xy_class2])\n",
    "    np.random.shuffle(data_full)\n",
    "    data = data_full[:, :2]\n",
    "    labels = data_full[:, 2]\n",
    "\n",
    "    print(\"###################plot of original data####################\")\n",
    "    plt.plot(x_class1[:, 0], x_class1[:, 1], 'X')  # first class, x shape\n",
    "    plt.plot(x_class2[:, 0], x_class2[:, 1], 'o')  # second class, circle shape\n",
    "    plt.show()\n",
    "\n",
    "    # Run k-mean\n",
    "    N, D = data.shape\n",
    "    K = 2\n",
    "    max_iter = 100\n",
    "    class_init = np.random.binomial(1., .5, size=N)\n",
    "    R = np.vstack([class_init, 1 - class_init]).T\n",
    "\n",
    "    Mu = np.zeros([D, K])\n",
    "    Mu[:, 1] = 1.\n",
    "    R.T.dot(data), np.sum(R, axis=0)\n",
    "\n",
    "    for it in range(max_iter):\n",
    "        R = km_assignment_step(data, Mu)\n",
    "        Mu = km_refitting_step(data, R, Mu)\n",
    "        print(it, cost(data, R, Mu))\n",
    "\n",
    "    class_1 = np.where(R[:, 0])\n",
    "    class_2 = np.where(R[:, 1])\n",
    "\n",
    "    correct_class_1 = num_samples // 2 - sum(labels[class_1])\n",
    "    correct_class_2 = sum(labels[class_2])\n",
    "    num_correct = correct_class_1 + correct_class_2\n",
    "    print('Missclassification rate of K-mean clustering is ', round(1-num_correct/400,3)*100, '%')\n",
    "    x_class1_new = data[class_1]\n",
    "    x_class2_new = data[class_2]\n",
    "    print(\"##########################Plot of k-mean#################################\")\n",
    "    plt.plot(x_class1_new[:, 0], x_class1_new[:, 1], 'X')  # first class, x shape\n",
    "    plt.plot(x_class2_new[:, 0], x_class2_new[:, 1], 'o')  # second class, circle shape\n",
    "    plt.show()\n",
    "\n",
    "    # Run EM algorithm\n",
    "    N, D = data.shape\n",
    "    K = 2\n",
    "    Mu = np.zeros([D, K])\n",
    "    Mu[:, 1] = 1.\n",
    "    Sigma = [np.eye(2), np.eye(2)]\n",
    "    Pi = np.ones(K) / K\n",
    "    Gamma = np.zeros([N, K]) # Gamma is the matrix of responsibilities\n",
    "\n",
    "    max_iter  = 200\n",
    "\n",
    "    for it in range(max_iter):\n",
    "        Gamma = gm_e_step(data, Mu, Sigma, Pi)\n",
    "        Mu, Sigma, Pi = gm_m_step(data, Gamma)\n",
    "        # print(it, log_likelihood(data, Mu, Sigma, Pi)) # This function makes the computation longer, but good for debugging\n",
    "\n",
    "    class_1 = np.where(Gamma[:, 0] >= .5)\n",
    "    class_2 = np.where(Gamma[:, 1] >= .5)\n",
    "\n",
    "    correct_class_1 = num_samples // 2 - sum(labels[class_1])\n",
    "    correct_class_2 = sum(labels[class_2])\n",
    "    num_correct = correct_class_1 + correct_class_2\n",
    "    print('Missclassification rate of EM algorithm is ', round(1-num_correct/400,3)*100, '%')\n",
    "    x_class1_new = data[class_1]\n",
    "    x_class2_new = data[class_2]\n",
    "    print(\"##########################Plot of EM algorithm#################################\")\n",
    "    plt.plot(x_class1_new[:, 0], x_class1_new[:, 1], 'X')  # first class, x shape\n",
    "    plt.plot(x_class2_new[:, 0], x_class2_new[:, 1], 'o')  # second class, circle shape\n",
    "    plt.show()\n",
    "\n",
    "    print(\"###########################End Experiement No.\",i+1,\"#######################################\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}